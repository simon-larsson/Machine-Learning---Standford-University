--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0. Neural Networks - Cost Function
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	COST FUNCTION
		
	- Logistic regression: J = (1/m)*(-(yi') * log(hθ(X)i) - (1 - y') * log(1 - hθ(X)i)) + regularization
			The difference here from regular log. reg. is that we have yi and hθ(X)i where i goes from 1 -> K.

			Regularization: (lambda/(2*m))*sum(θji.^2) where j = 0 is exluded
			
	- Linear regression is same as before but with the same change with y and 0 getting an extra dimension i
			
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 1. Neural Networks - Back Propagation
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	COMPUTATION FORMULAS
	
		- δj^(l) = "error" of node j in layer l
	
		- Iterative formula calculating δ^(l) with back propagation:
		|--------------------------------------------------------------------------------------------
		|	
		|	Init 			δ^(L) = a^(L) - y
		|				
		|	Iterate 		δ^(i - 1) = (θ^(i))' * δ^(i - 1) .* diff(g(z^(i - 1)))
		|	
		|					diff(g(z^(i - 1))) = a^(i - 1) .* (1 - a^(i - 1))
		|	
		|	Stop at 		δ^(2)       (δ^(1) is the inputs and no "error" is calculated there) 
		|	
		|--------------------------------------------------------------------------------------------	
			
		- Gradients: diff(J(θij^(l))) = aj^(l) * δi^(l + 1)		(without regularization)
			
			
			
		- Forward and back propagation formula:
		|--------------------------------------------------------------------------------------------
		|	Set	Δij^(l) = 0 (for all l, i, j).
		|	
		|	For i = 1 to m
		|		
		|		Set a^(1) = x^(i)
		|		
		|		Perform forward prop to compute a^(l) l = 2,3, ... , L
		|		
		|		Using y^(i), compute δ^(L) = a^(L) - y^(i)
		|		
		|		Compute δ^(L - 1), δ^(L - 2), ..... δ^(2)
		|		
		|		Perform forward prop:	Δ^(l) := Δ^(l) + δ^(l + 2) * a^(l)' 		- Vectorized
		|										Δij^(l) := Δij^(l) + aj^(l) * δ^(l + 2)	- Unvectorized
		|	end
		|	
		|	Dij^(l) := (1/m) * Δij^(l) +  λ*θij^(l)	if j != 0	|	where Dij^(l) = diff(J(θij^(l)))
		| 	Dij^(l) := (1/m) * Δij^(l) 					if j = 0
		|		
		|--------------------------------------------------------------------------------------------
		
			
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


	PARAMETER UNROLLING
		
		- The optimization algorithms, such as fminunc, are expecting parameter vectors.
		
		- Linear/logistic regression used θ which were parameter vectors
		
		- Neural networks use θ which are parameter matrices
		
		- Same goes for the gradients D
		
		- To solve this parameter unrolling us performed
		
		- Parameter unrolling example:
		|--------------------------------------------------------------------------------------------
		|  θ1, θ2, θ3 and D1, D2, D3 which are all matrices
		|
		|	Dimensions: s1 = 10, s2 = 10, s3 = 1	θ, D = 10x11 except the output which is 1x11
		|	
		| 	thetaVec = [ Theta1(:) ; Theta2(:) ; Theta3(:) ]
		| 	DVec 		= [ D1(:) ; D2(:) ; D3(:) ]
		|
		|--------------------------------------------------------------------------------------------
			
		- Restore matrices after unrolling
		|--------------------------------------------------------------------------------------------
		|
		|  Theta1 = reshape(thetaVec(1:110), 10, 11)
		|	Theta2 = reshape(thetaVec(111:220), 10, 11)
		| 	Theta3 = reshape(thetaVec(221:231), 1, 11)
		| 	
		|	D1 = reshape(DVec(1:110), 10, 11)
		|	D2 = reshape(DVec(111:220), 10, 11)
		| 	D3 = reshape(DVec(221:231), 1, 11)
		|
		|--------------------------------------------------------------------------------------------
						
function [m,s] = stat2(x)
n = length(x);
m = avg(x,n);
s = sqrt(sum((x-m).^2/n));
end	
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
				
	GRADIENT CHECKING
	
		- When using backprop it is easy have bugs that are hard to find since the error might still decrease, 
			to remedy this gradient checking is used.
	
		- Calc the on the unrolled θ derivative definition of J(θ) 	where ε ~= 10^(-4)
		
		- Formula:    (J(θ + ε) - J(θ - ε)) / (2*ε)	
		
		- This derivative is the same as what the backprop later will approximate. Therefore with can compared 
			with the D-gradient from backprop to confirm that backprop is working.
		
		Implementation:
		|--------------------------------------------------------------------------------------------
		|
		|  for i = 1:n
		|		thetaPlus = theta;
		| 		thetaPlus(i) = thetaPlus(i) + EPSILON;
		|		thetaMinus = theta; 	
		|		thetaMinus(i) = thetaMinus(i) - EPSILON;
		|		gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2*EPSILON);
		| 	end;
		|
		|--------------------------------------------------------------------------------------------
		|	Check gradApprox ~= DVec to confirm backprop is correct
		|--------------------------------------------------------------------------------------------
			
			
		- Gradient checking will make the code slow. Should only be implemented when developing backprop to confirm
			and later be turned off.
	

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	RANDOM INITIALIZATION

		- Setting initial θ to zero does not work for neural networks since it will make all units in a layer identical.
			It will make the network only have one feature.
		
		- Random initialization implemention:
		|--------------------------------------------------------------------------------------------
		|	Theta1 = rand({dimension1}) * (2*INIT_EPSILON) 	- INIT_EPSILON
		|	Theta2 = rand({dimension2}) * (2*INIT_EPSILON) 	- INIT_EPSILON
		|--------------------------------------------------------------------------------------------


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 3. Neural Networks - Putting it together
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	BUILDING A NETWORK

	 - Network architecture
		Number of input units:					Dimension of the features x^(i)
		Number of output units:					Number of classes (if multiclass)
		Number of hidden layers:				Default 1
		Number of units in hidden layer:		Same of every layer, usually the more the better
	
	- Performing forwardprop and backprop implementation:
		|--------------------------------------------------------------------------------------------
		|	for i = 1:m
		|		Perform forwardprop using x^(i), y^(i) to get a^(l) terms
		|		
		|		Perform backprop using x^(i), y^(i) to get  δ^(l) terms
		|
		|		Gather Δ terms: Δ^(l) := Δ^(l) + δ^(l + 2) * a^(l)'
		|
		|	end
		|	
		|	Calcuate Dij^(l) = diff(J(θij^(l)))
		|--------------------------------------------------------------------------------------------
	
	- Implementation steps
		1. Randomly initialize weights
		2. Implement forwardprop to get hθ(x^(i)) for any x^(i)
		3. Implement computation of cost function J(θ)
		4. Implement backprop to compute partial derivatives of the J(θ) in relation to θjk^(l)
		5. Perform forwardprop and backprop that uses x^(i), y^(i)
		6. Perform gradient checking
		7. Use optimization method with backprop to minimize J(θ)
		

	
	